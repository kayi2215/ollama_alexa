{
  "models": {
    "llama3": {
      "parameters": {
        "temperature": 0.7,
        "top_p": 0.9,
        "max_tokens": 500,
        "presence_penalty": 0,
        "frequency_penalty": 0
      }
    }
  },
  "api": {
    "host": "0.0.0.0",
    "port": 11434,
    "cors_origins": ["*"],
    "timeout": 120
  },
  "gpu": {
    "layers": -1
  }
}